{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998703c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93330d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/Sample_Submission.xlsx\n",
      "kaggle/Training_Dataset.xlsx\n",
      "kaggle/Testing_Dateset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('kaggle'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac7d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm openpyxl -q\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85dd04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PURCHASE INTENT PREDICTION - DUAL OUTPUT PIPELINE\n",
      "============================================================\n",
      "This pipeline creates two submission files:\n",
      "1. submission_probability.csv - Probability predictions (0-1)\n",
      "2. submission_binary.csv - Binary predictions (0 or 1)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PURCHASE INTENT PREDICTION - DUAL OUTPUT PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This pipeline creates two submission files:\")\n",
    "print(\"1. submission_probability.csv - Probability predictions (0-1)\")\n",
    "print(\"2. submission_binary.csv - Binary predictions (0 or 1)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d86939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading datasets...\n",
      "Training data shape: (177086, 23)\n",
      "Test data shape: (44272, 23)\n",
      "Sample submission shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nStep 1: Loading datasets...\")\n",
    "train_df = pd.read_excel('kaggle/Training_Dataset.xlsx')\n",
    "test_df = pd.read_excel('kaggle/Testing_Dateset.xlsx')\n",
    "sample_submission = pd.read_excel('kaggle/sample_Submission.xlsx')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f106b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test IDs corrected: 177087 to 221358\n"
     ]
    }
   ],
   "source": [
    "# Correct test IDs to continue from training data\n",
    "test_df['ID'] = range(177087, 177087 + len(test_df))\n",
    "print(f\"Test IDs corrected: {test_df['ID'].min()} to {test_df['ID'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42071da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target distribution: Class 0=101421, Class 1=75665\n",
      "Class balance: 42.7% positive class\n"
     ]
    }
   ],
   "source": [
    "# Extract and validate target variable\n",
    "y = train_df['Result'].values.astype(int)\n",
    "print(f\"\\nTarget distribution: Class 0={sum(y==0)}, Class 1={sum(y==1)}\")\n",
    "print(f\"Class balance: {sum(y==1)/len(y)*100:.1f}% positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "935e7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Date features\n",
    "    for col in ['Start Date', 'End Date']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    if 'Start Date' in df.columns and 'End Date' in df.columns:\n",
    "        _extracted_from_engineer_features_10(df)\n",
    "\n",
    "\n",
    "# TODO Rename this here and in `engineer_features`\n",
    "def _extracted_from_engineer_features_10(df):\n",
    "    df['Journey_Duration'] = (df['End Date'] - df['Start Date']).dt.days\n",
    "    df['Journey_Duration'] = df['Journey_Duration'].fillna(0).clip(lower=0, upper=365)\n",
    "\n",
    "    df['Start_Year'] = df['Start Date'].dt.year.fillna(2021)\n",
    "    df['Start_Month'] = df['Start Date'].dt.month.fillna(6)\n",
    "    df['Start_Quarter'] = df['Start Date'].dt.quarter.fillna(2)\n",
    "    df['Start_DayOfWeek'] = df['Start Date'].dt.dayofweek.fillna(3)\n",
    "\n",
    "    df['End_Year'] = df['End Date'].dt.year.fillna(2021)\n",
    "    df['End_Month'] = df['End Date'].dt.month.fillna(6)\n",
    "    df['End_Quarter'] = df['End Date'].dt.quarter.fillna(2)\n",
    "    df['End_DayOfWeek'] = df['End Date'].dt.dayofweek.fillna(3)\n",
    "\n",
    "    df = df.drop(['Start Date', 'End Date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4ae84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading datasets...\n",
      "Training data shape: (177086, 23)\n",
      "Test data shape: (44272, 23)\n",
      "Sample submission shape: (4, 2)\n",
      "Test IDs corrected: 177087 to 221358\n",
      "\n",
      "Target distribution: Class 0=101421, Class 1=75665\n",
      "Class balance: 42.7% positive class\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nStep 1: Loading datasets...\")\n",
    "train_df = pd.read_excel('kaggle/Training_Dataset.xlsx')\n",
    "test_df = pd.read_excel('kaggle/Testing_Dateset.xlsx')\n",
    "sample_submission = pd.read_excel('kaggle/Sample_Submission.xlsx')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Correct test IDs to continue from training data\n",
    "test_df['ID'] = range(177087, 177087 + len(test_df))\n",
    "print(f\"Test IDs corrected: {test_df['ID'].min()} to {test_df['ID'].max()}\")\n",
    "\n",
    "# Extract and validate target variable\n",
    "y = train_df['Result'].values.astype(int)\n",
    "print(f\"\\nTarget distribution: Class 0={sum(y==0)}, Class 1={sum(y==1)}\")\n",
    "print(f\"Class balance: {sum(y==1)/len(y)*100:.1f}% positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feee1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Date features\n",
    "    for col in ['Start Date', 'End Date']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    start_col = 'Start Date'\n",
    "    end_col = 'End Date'\n",
    "    if start_col in df.columns and end_col in df.columns:\n",
    "        df['Journey_Duration'] = (df[end_col] - df[start_col]).dt.days\n",
    "        df['Journey_Duration'] = df['Journey_Duration'].fillna(0).clip(lower=0, upper=365)\n",
    "        \n",
    "        df['Start_Year'] = df[start_col].dt.year.fillna(2021)\n",
    "        df['Start_Month'] = df[start_col].dt.month.fillna(6)\n",
    "        df['Start_Quarter'] = df[start_col].dt.quarter.fillna(2)\n",
    "        df['Start_DayOfWeek'] = df[start_col].dt.dayofweek.fillna(3)\n",
    "        \n",
    "        df['End_Year'] = df[end_col].dt.year.fillna(2021)\n",
    "        df['End_Month'] = df[end_col].dt.month.fillna(6)\n",
    "        df['End_Quarter'] = df[end_col].dt.quarter.fillna(2)\n",
    "        df['End_DayOfWeek'] = df[end_col].dt.dayofweek.fillna(3)\n",
    "        \n",
    "        df = df.drop([start_col, end_col], axis=1)\n",
    "    \n",
    "    # Numerical features\n",
    "    if 'Price' in df.columns:\n",
    "        price_median = df['Price'].median()\n",
    "        df['Price'] = df['Price'].fillna(price_median)\n",
    "        df['Price_Log'] = np.log1p(df['Price'].clip(lower=0))\n",
    "        df['Price_Sqrt'] = np.sqrt(df['Price'].clip(lower=0))\n",
    "    \n",
    "    if 'Estimated Win Rate' in df.columns:\n",
    "        df['Estimated Win Rate'] = df['Estimated Win Rate'].fillna(0.5)\n",
    "        df['Win_Rate_Squared'] = df['Estimated Win Rate'] ** 2\n",
    "        df['Win_Rate_High'] = (df['Estimated Win Rate'] > 0.75).astype(int)\n",
    "        df['Win_Rate_Low'] = (df['Estimated Win Rate'] < 0.25).astype(int)\n",
    "    \n",
    "    if 'Unit Number' in df.columns:\n",
    "        df['Unit Number'] = df['Unit Number'].fillna(0)\n",
    "        df['Unit_Log'] = np.log1p(df['Unit Number'])\n",
    "        df['Has_Units'] = (df['Unit Number'] > 0).astype(int)\n",
    "        df['Multiple_Units'] = (df['Unit Number'] > 1).astype(int)\n",
    "    \n",
    "    # Customer segments\n",
    "    segment_cols = [f'Customer Segment {i}' for i in range(1, 6)]\n",
    "    if (existing_segments := [col for col in segment_cols if col in df.columns]):\n",
    "        df['Active_Segments'] = df[existing_segments].notna().sum(axis=1)\n",
    "    \n",
    "    # Manager features\n",
    "    if 'Manager' in df.columns:\n",
    "        df['Has_Manager'] = df['Manager'].notna().astype(int)\n",
    "    if 'Techincal Manager' in df.columns:\n",
    "        df['Has_Tech_Manager'] = df['Techincal Manager'].notna().astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29490132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Engineering features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2: Engineering features...\")\n",
    "train_featured = engineer_features(train_df)\n",
    "test_featured = engineer_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e693b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['ID', 'Result', 'Customer ID']\n",
    "feature_cols = [col for col in train_featured.columns if col not in exclude_cols]\n",
    "\n",
    "X = train_featured[feature_cols].copy()\n",
    "X_test = test_featured[feature_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccbdafee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: 38 common features\n",
      "\n",
      "Step 3: Encoding categorical variables...\n",
      "Categorical columns to encode: 15\n"
     ]
    }
   ],
   "source": [
    "# Align columns\n",
    "common_cols = sorted(list(set(X.columns) & set(X_test.columns)))\n",
    "X = X[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "print(f\"Features selected: {len(common_cols)} common features\")\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"\\nStep 3: Encoding categorical variables...\")\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns to encode: {len(categorical_cols)}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = X[col].fillna('missing').astype(str)\n",
    "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
    "    all_values = pd.concat([X[col], X_test[col]]).unique()\n",
    "    le.fit(all_values)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b661bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Handle remaining missing values\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        median_val = X[col].median()\n",
    "        X[col] = X[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4081f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes - Training: (177086, 38), Test: (44272, 38)\n"
     ]
    }
   ],
   "source": [
    "# Convert to arrays\n",
    "X = X.values.astype(np.float32)\n",
    "X_test = X_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"Final shapes - Training: {X.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72eb65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Training models...\n",
      "\n",
      "Training RandomForest...\n",
      "  Validation AUC: 0.9978\n",
      "  Validation Accuracy: 0.9909\n",
      "\n",
      "Training XGBoost...\n",
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9916\n",
      "\n",
      "Training LightGBM...\n",
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9914\n",
      "\n",
      "Training GradientBoosting...\n",
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "print(\"\\nStep 4: Training models...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=12, min_samples_split=15,\n",
    "        min_samples_leaf=8, max_features='sqrt',\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=150, max_depth=6, learning_rate=0.08,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        n_estimators=150, max_depth=6, learning_rate=0.08,\n",
    "        num_leaves=40, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, verbose=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=150, max_depth=5, learning_rate=0.08,\n",
    "        subsample=0.8, max_features='sqrt',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "validation_scores = {}\n",
    "test_probabilities = {}\n",
    "test_binary = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "        # Validation performance\n",
    "    val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    val_binary = model.predict(X_val)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, val_prob)\n",
    "    acc = accuracy_score(y_val, val_binary)\n",
    "    validation_scores[name] = auc\n",
    "    \n",
    "    # Test predictions - both probability and binary\n",
    "    test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    test_bin = model.predict(X_test)\n",
    "    \n",
    "    test_probabilities[name] = test_prob\n",
    "    test_binary[name] = test_bin\n",
    "    \n",
    "    print(f\"  Validation AUC: {auc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df15a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc965650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Creating ensemble predictions...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create ensembles\n",
    "print(\"\\nStep 5: Creating ensemble predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0027829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights based on AUC:\n",
      "  XGBoost: 0.250\n",
      "  LightGBM: 0.250\n",
      "  GradientBoosting: 0.250\n",
      "  RandomForest: 0.250\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights based on AUC scores\n",
    "total_score = sum(validation_scores.values())\n",
    "weights = {name: score/total_score for name, score in validation_scores.items()}\n",
    "\n",
    "print(\"Model weights based on AUC:\")\n",
    "for name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {weight:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "452c1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted ensemble for probabilities\n",
    "ensemble_prob = np.zeros(len(X_test))\n",
    "for name, weight in weights.items():\n",
    "    ensemble_prob += test_probabilities[name] * weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e9f2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority voting for binary predictions\n",
    "ensemble_binary_votes = np.array(list(test_binary.values()))\n",
    "ensemble_binary = (ensemble_binary_votes.sum(axis=0) >= len(models)/2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44a5702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Retraining on full dataset...\n",
      "Retraining RandomForest...\n",
      "Retraining XGBoost...\n",
      "Retraining LightGBM...\n",
      "Retraining GradientBoosting...\n"
     ]
    }
   ],
   "source": [
    "# Retrain on full data for final predictions\n",
    "print(\"\\nStep 6: Retraining on full dataset...\")\n",
    "final_probabilities = []\n",
    "final_binary_predictions = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Retraining {name}...\")\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    prob = model.predict_proba(X_test)[:, 1]\n",
    "    binary = model.predict(X_test)\n",
    "    \n",
    "    final_probabilities.append(prob)\n",
    "    final_binary_predictions.append(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final ensembles\n",
    "final_ensemble_prob = np.mean(final_probabilities, axis=0)\n",
    "final_ensemble_binary = (np.array(final_binary_predictions).sum(axis=0) >= len(models)/2).astype(int)\n",
    "print(\"\\nStep 7: Creating submission files...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c348cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission 1: Probability predictions (for ROC AUC evaluation)\n",
    "submission_probability = pd.DataFrame({\n",
    "    'ID': test_featured['ID'].values,\n",
    "    'TARGET': final_ensemble_prob\n",
    "})\n",
    "submission_probability['TARGET'] = submission_probability['TARGET'].clip(0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42f14071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission 2: Binary predictions\n",
    "submission_binary = pd.DataFrame({\n",
    "    'ID': test_featured['ID'].values,\n",
    "    'TARGET': final_ensemble_binary\n",
    "})\n",
    "\n",
    "# Save both files\n",
    "submission_probability.to_csv('submission_probability.csv', index=False)\n",
    "submission_binary.to_csv('submission_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6938e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUBMISSION FILE 1: PROBABILITY PREDICTIONS\n",
      "File name: submission_probability.csv\n",
      "Shape: (44272, 2)\n",
      "TARGET range: [0.0001, 0.9973]\n",
      "TARGET mean: 0.3056\n",
      "TARGET std: 0.4542\n",
      "\n",
      "First 5 rows:\n",
      "       ID    TARGET\n",
      "0  177087  0.990085\n",
      "1  177088  0.021236\n",
      "2  177089  0.000282\n",
      "3  177090  0.002150\n",
      "4  177091  0.000312\n",
      "\n",
      "============================================================\n",
      "SUBMISSION FILE 2: BINARY PREDICTIONS\n",
      "File name: submission_binary.csv\n",
      "Shape: (44272, 2)\n",
      "Class distribution: 0=30579, 1=13693\n",
      "Positive class percentage: 30.9%\n",
      "\n",
      "First 5 rows:\n",
      "       ID  TARGET\n",
      "0  177087       1\n",
      "1  177088       0\n",
      "2  177089       0\n",
      "3  177090       0\n",
      "4  177091       0\n"
     ]
    }
   ],
   "source": [
    "# Display statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION FILE 1: PROBABILITY PREDICTIONS\")\n",
    "print(\"File name: submission_probability.csv\")\n",
    "print(f\"Shape: {submission_probability.shape}\")\n",
    "print(f\"TARGET range: [{submission_probability['TARGET'].min():.4f}, {submission_probability['TARGET'].max():.4f}]\")\n",
    "print(f\"TARGET mean: {submission_probability['TARGET'].mean():.4f}\")\n",
    "print(f\"TARGET std: {submission_probability['TARGET'].std():.4f}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(submission_probability.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION FILE 2: BINARY PREDICTIONS\")\n",
    "print(\"File name: submission_binary.csv\")\n",
    "print(f\"Shape: {submission_binary.shape}\")\n",
    "print(f\"Class distribution: 0={sum(submission_binary['TARGET']==0)}, 1={sum(submission_binary['TARGET']==1)}\")\n",
    "print(f\"Positive class percentage: {sum(submission_binary['TARGET']==1)/len(submission_binary)*100:.1f}%\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(submission_binary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Step 1: Loading datasets...\n",
      "Training data shape: (177086, 23)\n",
      "Test data shape: (44272, 23)\n",
      "Sample submission shape: (4, 2)\n",
      "Test IDs corrected: 177087 to 221358\n",
      "\n",
      "Target distribution: Class 0=101421, Class 1=75665\n",
      "Class balance: 42.7% positive class\n",
      "\n",
      "Step 2: Engineering features...\n",
      "Features selected: 38 common features\n",
      "\n",
      "Step 3: Encoding categorical variables...\n",
      "Categorical columns to encode: 15\n",
      "Final shapes - Training: (177086, 38), Test: (44272, 38)\n",
      "\n",
      "Step 4: Training models...\n",
      "\n",
      "Training RandomForest...\n",
      "  Validation AUC: 0.9978\n",
      "  Validation Accuracy: 0.9909\n",
      "\n",
      "Training XGBoost...\n",
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9916\n",
      "\n",
      "Training LightGBM...\n",
      "  Validation AUC: 0.9979\n",
      "  Validation Accuracy: 0.9914\n",
      "\n",
      "Training GradientBoosting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# Validation performance\u001b[39;00m\n\u001b[32m    171\u001b[39m     val_prob = model.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/tree/_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Comparison visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(submission_probability['TARGET'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(['Class 0', 'Class 1'], \n",
    "        [sum(submission_binary['TARGET']==0), sum(submission_binary['TARGET']==1)],\n",
    "        color=['coral', 'lightgreen'])\n",
    "plt.title('Binary Prediction Distribution')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "threshold = 0.5\n",
    "binary_from_prob = (submission_probability['TARGET'] > threshold).astype(int)\n",
    "confusion = pd.crosstab(submission_binary['TARGET'], binary_from_prob, \n",
    "                        rownames=['Binary Ensemble'], colnames=['Prob > 0.5'])\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Binary vs Probability (>0.5) Agreement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"\\nTwo submission files created:\")\n",
    "print(\"1. submission_probability.csv - Use this for ROC AUC evaluation\")\n",
    "print(\"2. submission_binary.csv - Binary classification output\")\n",
    "print(\"\\nThe competition requires probability predictions for ROC AUC scoring.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bac9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63861b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
